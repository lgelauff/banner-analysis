{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import datetime\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T, Window\n",
    "import wmfdata.spark as wmfspark\n",
    "\n",
    "## defining the spark session\n",
    "spark_config = {}\n",
    "spark = wmfspark.get_session(\n",
    "    app_name='Pyspark notebook', \n",
    "    type='regular'\n",
    "#     extra_settings=spark_config\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parsing uri-query field in webrequest\n",
    "parse_uri_query = \"\"\"parse_url(concat('http://bla.org/woo/', uri_query), 'QUERY', '{0}')\"\"\"\n",
    "## hashing user-agent and client-ip (with salt) to generate pseudo user-id\n",
    "parse_user_id = \"\"\"sha2(CONCAT(user_agent, client_ip, '{0}'), 256)\"\"\" \n",
    "# salts for UA/IP hash (1st = userhash one day) \n",
    "# from https://github.com/geohci/covid-19-sessions/blob/master/covid_19_data.ipynb\n",
    "salt_one = ''.join(random.choice(string.ascii_lowercase + string.ascii_uppercase + string.digits) for _ in range(random.randint(8,16)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the pageviews of all user that had banner-loads\n",
    "\n",
    "Once we have all requests to banner-impressions, we can also look up all the requests to pageviews by the same user-id.\n",
    "\n",
    "- get all wlm-banner loads\n",
    "- get all users associated with a wlm-banner loads\n",
    "- get pageviews in all projects by the same user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executing, use the following functions:\n",
    "- get_day_of_data, this gives a pd dataframe with edited columns\n",
    "- clean_data_make_csv, this creates the csv files\n",
    "- get the user aggregates (currently not in a function, but under 'execute' the query is outlined)\n",
    "- delete the parquet folders (currently manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## windowing function\n",
    "w = Window.partitionBy(F.col('user_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parquet_paths(\n",
    "    year = 2021,\n",
    "    month = 10,\n",
    "    day=1,\n",
    "    folder = 'output/',\n",
    "    filename_main = 'day_of_data_',\n",
    "    skiptestlocal = False\n",
    "):\n",
    "    main_output = folder + filename_main\n",
    "    PATH_hadoop = \"/user/effeietsanders/\"\n",
    "    FILE_hadoop = main_output + str(year) + str(month).zfill(2) + str(day).zfill(2) + '.parquet'\n",
    "    PATH_local = \"/home/effeietsanders/shared_notebooks/\"\n",
    "    FILE_local = main_output + str(year) + str(month).zfill(2) + str(day).zfill(2) + '.parquet'\n",
    "    filename = filename_main + str(year) + str(month).zfill(2) + str(day).zfill(2) + '.parquet'\n",
    "    localfile_exists = os.path.isdir(PATH_local + FILE_local)\n",
    "    if (localfile_exists):\n",
    "        if (skiptestlocal):\n",
    "            pass\n",
    "        else:\n",
    "            print(datetime.datetime.now(), \"file already exists locally: \", PATH_local + FILE_local)\n",
    "            sys.exit(\"file already exists locally\")\n",
    "    hadoopfile_exists = os.system(\"hadoop fs -ls %s\" % (PATH_hadoop + FILE_hadoop) )\n",
    "    if (hadoopfile_exists == 0):\n",
    "        print(datetime.datetime.now(), \"file already exist on hadoop\")\n",
    "        sys.exit(\"file already exist on hadoop\")\n",
    "    print(datetime.datetime.now(), \"strings set\")\n",
    "    return(\n",
    "        PATH_hadoop, \n",
    "        FILE_hadoop,\n",
    "        PATH_local,\n",
    "        FILE_local,\n",
    "        filename\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of get_pd_df2 but then try without toPandas()\n",
    "def get_pq(\n",
    "    query_year = 2021,\n",
    "    query_month = 10,\n",
    "    query_day = 1,\n",
    "#     query_hour = 1,\n",
    "    FILE_hadoop='output/testing.parquet'\n",
    "):\n",
    "    df = (\n",
    "        spark.read.table(\"wmf.webrequest\")\n",
    "        ## specify time-window (snapshot)\n",
    "        .where(F.col(\"year\")==query_year)\n",
    "        .where(F.col(\"month\")==query_month)\n",
    "        .where(F.col(\"day\")==query_day)\n",
    "#         .where(F.col(\"hour\")==query_hour)\n",
    "\n",
    "        ## generate user id\n",
    "        .withColumn(\"user_id\", F.expr(parse_user_id.format(salt_one)) )\n",
    "\n",
    "        ## agent-type user to filter spiders\n",
    "        ## https://meta.wikimedia.org/wiki/Research:Page_view/Tags#Spider\n",
    "        .where(F.col(\"agent_type\") == \"user\")\n",
    "        .where(F.col(\"webrequest_source\") == \"text\")\n",
    "        ## only users that are not logged in?\n",
    "        .withColumn('logged_in', F.coalesce(F.col('x_analytics_map.loggedIn'),F.lit(0)) )\n",
    "#         .where( F.col('logged_in') == 0 )    \n",
    "        ## drop requests with no timestamps\n",
    "        .where(F.col(\"dt\")!='-')\n",
    "\n",
    "        ## select banner-impressions or pageviews\n",
    "        ## or special pages CreateAccount or Upload or UploadWizard\n",
    "        .withColumn(\"uri_title\",F.expr(parse_uri_query.format(\"title\")))\n",
    "        \n",
    "        .where(\n",
    "            (F.col(\"is_pageview\")==1)|\\\n",
    "            F.col(\"uri_title\").isin(\n",
    "                \"Special:BannerLoader\",\n",
    "                \"Special:CreateAccount\",\n",
    "                \"Սպասարկող:CreateAccount\",\n",
    "                \"Xüsusi:HesabAç\",\n",
    "                \"Especial:Criar_conta\",\n",
    "                \"Spezial:Benutzerkonto_anlegen\",\n",
    "                \"מיוחד:הרשמה_לחשבון\",\n",
    "                \"ویژه:ایجاد_حساب_کاربری\",\n",
    "                \"Posebno:Stvori_račun\",\n",
    "                \"Special:Înregistrare\",\n",
    "                \"Специјална:СоздајКорисничкаСметка\",\n",
    "                \"Служебная:Создать_учётную_запись\",\n",
    "                \"Спеціальна:Створити_обліковий_запис\",\n",
    "                \"Special:Upload\", \n",
    "                \"Special:UploadWizard\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## create columns for campaign-properties\n",
    "        .withColumn(\"bl_campaign\", F.expr(parse_uri_query.format(\"campaign\")))\n",
    "        .withColumn(\"bl_banner\", F.expr(parse_uri_query.format(\"banner\")))\n",
    "#         .withColumn(\"bl_uselang\", F.expr(parse_uri_query.format(\"uselang\")))\n",
    "\n",
    "        ## create a column to indicate whether a request came from a wlm-banner impression (otherwise None)\n",
    "        .withColumn(\n",
    "            \"bi_iswlm\", \n",
    "            F.when(\n",
    "                F.col(\"bl_banner\").startswith(\"wlm_2021\"),\n",
    "                1\n",
    "            ).otherwise(None)\n",
    "        )\n",
    "\n",
    "        .withColumn(\n",
    "            \"sp_createaccount\",\n",
    "            F.when(\n",
    "                F.col(\"uri_title\").isin(\n",
    "                    \"Special:CreateAccount\",\n",
    "                    \"Սպասարկող:CreateAccount\",\n",
    "                    \"Xüsusi:HesabAç\",\n",
    "                    \"Especial:Criar_conta\",\n",
    "                    \"Spezial:Benutzerkonto_anlegen\",\n",
    "                    \"מיוחד:הרשמה_לחשבון\",\n",
    "                    \"ویژه:ایجاد_حساب_کاربری\",\n",
    "                    \"Posebno:Stvori_račun\",\n",
    "                    \"Special:Înregistrare\",\n",
    "                    \"Специјална:СоздајКорисничкаСметка\",\n",
    "                    \"Служебная:Создать_учётную_запись\",\n",
    "                    \"Спеціальна:Створити_обліковий_запис\"\n",
    "                ),\n",
    "                1\n",
    "            ).otherwise(None)\n",
    "        )\n",
    "\n",
    "        .withColumn(\n",
    "            \"sp_upload\",\n",
    "            F.when(\n",
    "                F.col(\"uri_title\").isin(\"Special:Upload\", \"Special:UploadWizard\"),\n",
    "                1\n",
    "            ).otherwise(None)\n",
    "        )\n",
    "\n",
    "        ## mark all requests from users that saw a wlm-banner\n",
    "        .withColumn('user_wlm', F.max(F.col(\"bi_iswlm\")).over(w))\n",
    "        ## keep all requests from users that saw a wlm-banner\n",
    "        .where(F.col(\"user_wlm\").isNotNull())\n",
    "\n",
    "        ## rename some columns\n",
    "        .withColumn(\"page_title\",F.col('pageview_info.page_title'))\n",
    "        .withColumn(\"country\",F.col('geocoded_data.country'))\n",
    "        .withColumn(\"project_family\",F.col(\"normalized_host.project_family\"))\n",
    "        .withColumn(\"project\",F.col(\"normalized_host.project\"))\n",
    "    )\n",
    "    # convert to pandas to make it easier to inspect and process\n",
    "    return(df.write.parquet(FILE_hadoop))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix bad encoding for AZ, IR, UA?\n",
    "\n",
    "landingpages = pd.DataFrame([\n",
    "    ['wikipedia', 'hy', 'Վիքիպեդիա:Վիքին_սիրում_է_հուշարձաններ_2021', 'am'],\n",
    "    ['wikipedia', 'az', 'Vikipediya:Viki_Abid%C9%99l%C9%99ri_Sevir_2021', 'az'],\n",
    "    ['external', 'external', 'http://wlm.wikimedia.rs.ba/', 'ba'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Benin', 'bj'],\n",
    "    ['wikipedia', 'pt', 'Wikipédia:Wiki_Loves_Monuments_2021/Brasil', 'br'],\n",
    "    ['wikipedia', 'de', 'Wikipedia:Wiki_Loves_Monuments_2021/Deutschland', 'de'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Algeria', 'dz'],\n",
    "    ['external', 'external', 'https://www.wikilov.es/es/Wiki_Loves_Monuments', 'es'],\n",
    "    ['external', 'external', 'http://wlm.wikimedia.fi/', 'fi'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_France', 'fr'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Ghana', 'gh'],\n",
    "    ['external', 'external', 'https://wlm.wikimedia.gr/', 'gr'],\n",
    "    ['wikipedia', 'he', 'ויקיפדיה:מיזמי_ויקיפדיה/ויקיפדיה_אוהבת_אתרי_מורשת/תחרות_צילומים', 'il'],\n",
    "    ['wikipedia', 'fa', 'ویکی%E2%80%8Cپدیا:ویکی_دوستدار_یادمان%E2%80%8Cها_۲۰۲۱_ایران', 'ir'],\n",
    "    ['wikipedia', 'hr', 'Wikipedija:Wiki_voli_spomenike', 'hr'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Ireland', 'ie'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_India', 'in'],\n",
    "    ['external', 'external', 'https://wikilovesmonuments.wikimedia.it/', 'it'],\n",
    "    ['wikipedia', 'ro', 'Wikipedia:Wiki_Loves_Monuments/Moldova', 'md'],\n",
    "    ['wikipedia', 'mk', 'Википедија:Вики_ги_сака_спомениците_2021/Македонија', 'mk'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Malta', 'mt'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Malaysia', 'my'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Peru', 'pe'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Pakistan', 'pk'],\n",
    "    ['external', 'external', 'https://wikimedia.pl/zabytki', 'pl'],\n",
    "    ['external', 'external', 'https://wikilovesmonuments.org.pt/', 'pt'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Qatar', 'qa'],\n",
    "    ['wikivoyage', 'ru', 'Wikivoyage:Вики_любит_памятники_2021', 'ru'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Rwanda', 'rw'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Sweden/sv', 'se'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Slovenia', 'si'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_El_Salvador', 'sv'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Taiwan', 'tw'],\n",
    "    ['wikipedia', 'uk', 'Вікіпедія:Вікі_любить_пам%27ятки', 'ua'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Uganda', 'ug'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_the_United_States', 'us'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Monuments_2021_in_Zimbabwe', 'zw'],\n",
    "    ['wikimedia', 'commons', 'Commons:Czech_Wiki_Photo', 'cz'],\n",
    "    ['wikimedia', 'commons', 'Commons:Wiki_Loves_Fashion_in_the_Philippines', 'ph']\n",
    "], columns=['project_family', 'project', 'page_title', 'country'])\n",
    "banners = [\n",
    "    ['wlm_2021_zw', 'zw'],\n",
    "    ['wlm_2021_us', 'us'],\n",
    "    ['wlm_2021_ug', 'ug'],\n",
    "    ['wlm_2021_ua', 'ua'],\n",
    "    ['wlm_2021_tw', 'tw'],\n",
    "    ['wlm_2021_sv', 'sv'],\n",
    "    ['wlm_2021_si', 'si'],\n",
    "    ['wlm_2021_se', 'se'],\n",
    "    ['wlm_2021_rw', 'rw'],\n",
    "    ['wlm_2021_ru', 'ru'],\n",
    "    ['wlm_2021_qa', 'qa'],\n",
    "    ['wlm_2021_pt', 'pt'],\n",
    "    ['wlm_2021_pl', 'pl'],\n",
    "    ['wlm_2021_PK', 'pk'],\n",
    "    ['wlm_2021_pe', 'pe'],\n",
    "    ['wlm_2021_my', 'my'],\n",
    "    ['wlm_2021_mt', 'mt'],\n",
    "    ['wlm_2021_mk', 'mk'],\n",
    "    ['wlm_2021_md', 'md'],\n",
    "    ['wlm_2021_it', 'it'],\n",
    "    ['wlm_2021_ir', 'ir'],\n",
    "    ['wlm_2021_in', 'in'],\n",
    "    ['wlm_2021_il', 'il'],\n",
    "    ['wlm_2021_ie', 'ie'],\n",
    "    ['wlm_2021_hr', 'hr'],\n",
    "    ['wlm_2021_gr', 'gr'],\n",
    "    ['wlm_2021_gh', 'gh'],\n",
    "    ['wlm_2021_fr', 'fr'],\n",
    "    ['wlm_2021_fi', 'fi'],\n",
    "    ['wlm_2021_es', 'es'],\n",
    "    ['wlm_2021_dz', 'dz'],\n",
    "    ['wlm_2021_de', 'de'],\n",
    "    ['wlm_2021_br', 'br'],\n",
    "    ['wlm_2021_bj', 'bj'],\n",
    "    ['wlm_2021_ba_srp', 'ba'],\n",
    "    ['wlm_2021_az', 'az'],\n",
    "    ['wlm_2021_am', 'am'],\n",
    "    ['wmcz_czech_wiki_photo_2021', 'cz'],\n",
    "    ['wmcz_czech_wiki_photo_2021_late', 'cz'],\n",
    "    ['WLFashion Philippines', 'ph']\n",
    "]\n",
    "# Data from https://petscan.wmflabs.org/?psid=20657414\n",
    "monument_lists_de = pd.read_csv('data/monument_lists_de.csv')\n",
    "monument_lists_de['project_family'] = 'wikipedia'\n",
    "monument_lists_de['project'] = 'de'\n",
    "\n",
    "# Data from https://petscan.wmflabs.org/?psid=20654082\n",
    "monument_lists_us = pd.read_csv('data/monument_lists_us.csv')\n",
    "monument_lists_us['project_family'] = 'wikipedia'\n",
    "monument_lists_us['project'] = 'en'\n",
    "\n",
    "# Set a maximum number of banner views that we report for anonymity\n",
    "banner_view_cap = 10.0\n",
    "landing_view_cap = 1\n",
    "listpg_seen_cap = 10\n",
    "\n",
    "def qualify_pages (df, returnme = True, lp=landingpages):\n",
    "    df.sort_values(by=\"ts\", inplace=True)\n",
    "    df['pg_landing'] = False\n",
    "    for i in landingpages.index:\n",
    "        df['pg_landing'] = df['pg_landing'] | (\n",
    "            df[['project_family', 'project', 'page_title']]==landingpages.loc[i, ['project_family', 'project', 'page_title']]\n",
    "        ).all(axis=1)\n",
    "    df['pg_commons_help'] = (df[['project_family', 'project', 'namespace_id']]==['wikimedia', 'commons', 6]).all(axis=1)\n",
    "    df['pg_commons_commons'] = (df[['project_family', 'project', 'namespace_id']]==['wikimedia', 'commons', 4]).all(axis=1)\n",
    "    \n",
    "    df['pg_list_de'] = pd.merge(\n",
    "        left=df,\n",
    "        right=monument_lists_de[['project_family', 'project', 'pageid']],\n",
    "        how='left',\n",
    "        left_on=['project_family', \"page_id\"],\n",
    "        right_on=['project_family', 'pageid'],\n",
    "        copy=False,\n",
    "        indicator=True\n",
    "    )['_merge'] == 'both'\n",
    "    df['pg_list_us'] = pd.merge(\n",
    "        left=df,\n",
    "        right=monument_lists_us[['project_family', 'project', 'pageid']],\n",
    "        how='left',\n",
    "        left_on=['project_family', \"page_id\"],\n",
    "        right_on=['project_family', 'pageid'],\n",
    "        copy=False,\n",
    "        indicator=True\n",
    "    )['_merge'] == 'both'\n",
    "    df['banners_seen'] = df.groupby(['user_id'])['bi_iswlm'].cumsum().fillna(method='ffill')\n",
    "    df.loc[df['banners_seen'] > banner_view_cap,'banners_seen'] = banner_view_cap\n",
    "    df['landing_seen'] = df.groupby(['user_id'])['pg_landing'].cumsum().fillna(method='ffill')\n",
    "    df.loc[df['landing_seen'] > landing_view_cap,'landing_seen'] = landing_view_cap\n",
    "    df['listpg_us_seen'] = df.groupby(['user_id'])['pg_list_us'].cumsum().fillna(method='ffill')\n",
    "    df.loc[df['listpg_us_seen'] > listpg_seen_cap,'listpg_us_seen'] = listpg_seen_cap\n",
    "    df['listpg_de_seen'] = df.groupby(['user_id'])['pg_list_de'].cumsum().fillna(method='ffill')\n",
    "    df.loc[df['listpg_de_seen'] > listpg_seen_cap,'listpg_de_seen'] = listpg_seen_cap\n",
    "    \n",
    "    if returnme:\n",
    "        return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_pd(PATH_local, FILE_local):\n",
    "    localfile_exists = os.path.isdir(PATH_local + FILE_local)\n",
    "    if (localfile_exists):\n",
    "        print(datetime.datetime.now(), \"file exists locally\")\n",
    "    else:\n",
    "        print(datetime.datetime.now(), \"file does not exist locally\")\n",
    "        sys.exit(\"file does not exist locally\")\n",
    "    print(datetime.datetime.now(), \"converting to pd\")\n",
    "    df_pd = pd.read_parquet(\n",
    "        path = FILE_local,\n",
    "        columns = [\n",
    "            'year',\n",
    "            'month',\n",
    "            'day',\n",
    "            'hour',\n",
    "            'ts',\n",
    "            'user_id',\n",
    "            'logged_in',\n",
    "            'is_pageview',\n",
    "            'access_method',\n",
    "            'referer_class',\n",
    "            'bl_campaign',\n",
    "            'bl_banner',\n",
    "            'bi_iswlm',\n",
    "            'sp_createaccount',\n",
    "            'sp_upload',\n",
    "            'user_wlm',\n",
    "            'namespace_id',\n",
    "            'page_title',\n",
    "            'page_id',\n",
    "            'project_family',\n",
    "            'project'\n",
    "        ]\n",
    "    )\n",
    "    print(datetime.datetime.now(), \"adding new columns\")\n",
    "    df_out = qualify_pages(df_pd)\n",
    "    print(datetime.datetime.now(), \"done with get_df_pd\")\n",
    "    return(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_of_data(\n",
    "    year=2021,\n",
    "    month=10,\n",
    "    day=1,\n",
    "    filename = 'day_of_data_'\n",
    "):\n",
    "    PATH_hadoop, FILE_hadoop, PATH_local, FILE_local, filename_set = set_parquet_paths(\n",
    "        year=year,\n",
    "        month=month,\n",
    "        day=day,\n",
    "        folder = 'output/',\n",
    "        filename_main = filename\n",
    "    )\n",
    "    get_pq(\n",
    "        query_year = year,\n",
    "        query_month = month,\n",
    "        query_day = day,\n",
    "        FILE_hadoop = FILE_hadoop\n",
    "    )\n",
    "    hadoopfile_exists = os.system(\"hadoop fs -ls %s\" % (PATH_hadoop + FILE_hadoop) )\n",
    "    if (hadoopfile_exists == 0):\n",
    "        print(datetime.datetime.now(), \"good: file exists on hadoop\")\n",
    "    else:\n",
    "        print(datetime.datetime.now(), \"does file exist on hadoop?\")\n",
    "        sys.exit(\"does file exist on hadoop?\")\n",
    "    transfer_success = os.system(\"hadoop fs -get %s %s\" % (PATH_hadoop + FILE_hadoop, PATH_local + FILE_local))\n",
    "    if (transfer_success == 0):\n",
    "        print(datetime.datetime.now(), \"good: file transferred\")\n",
    "    else:\n",
    "        print(datetime.datetime.now(), \"was transfer successful?\")\n",
    "        sys.exit(\"was transfer successful?\")\n",
    "    delete_success = os.system('hadoop fs -rm -r ' + PATH_hadoop + FILE_hadoop)\n",
    "    if (delete_success == 0):\n",
    "        print(datetime.datetime.now(), \"good: file on hadoop deleted\")\n",
    "    else:\n",
    "        print(datetime.datetime.now(), \"was deletion on hadoop successful?\")\n",
    "        sys.exit(\"was deletion on hadoop successful?\")\n",
    "    df_pd = get_df_pd(\n",
    "        PATH_local = PATH_local,\n",
    "        FILE_local = FILE_local\n",
    "    )\n",
    "    print(datetime.datetime.now(), \"done with get_day_of_data\")\n",
    "    return(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with only banner impressions\n",
    "bi_cols_out = [\n",
    "    'year', 'month', 'day', \n",
    "    'access_method', #mobile web, desktop, mobile app\n",
    "    'bl_campaign', 'bl_banner', #should contain same information\n",
    "    'logged_in', #binary\n",
    "    'referer_class', # 'internal', 'external (search engine)', 'none', 'external'\n",
    "    'project_family', 'project',\n",
    "    'user_wlm', #sanity check, should always be 1\n",
    "    'banners_seen', #count with cap at 10\n",
    "    'landing_seen' #count with cap at 1\n",
    "              ]\n",
    "\n",
    "lp_cols_out = [\n",
    "    'year', 'month', 'day', \n",
    "    'access_method', #mobile web, desktop, mobile app\n",
    "    'logged_in', #binary\n",
    "    'referer_class', # 'internal', 'external (search engine)', 'none', 'external'\n",
    "    'project_family', 'project',\n",
    "    'page_title',\n",
    "    'user_wlm', #sanity check, should always be 1\n",
    "    'banners_seen', #count with cap at 10\n",
    "    'landing_seen' #sanity check, should always be 1\n",
    "]\n",
    "\n",
    "ca_cols_out = [\n",
    "    'year', 'month', 'day', \n",
    "    'access_method', #mobile web, desktop, mobile app\n",
    "    'logged_in', #sanity check, should be 0 mostly \n",
    "    'referer_class', # 'internal', 'external (search engine)', 'none', 'external'\n",
    "    'project_family', 'project',\n",
    "    'sp_createaccount', #sanity check, should always be 1\n",
    "    'user_wlm', #sanity check, should always be 1\n",
    "    'banners_seen', #count with cap at 10\n",
    "    'landing_seen' #sanity check, should be 1 mostly\n",
    "]\n",
    "\n",
    "up_cols_out = [\n",
    "    'year', 'month', 'day', \n",
    "    'access_method', #mobile web, desktop, mobile app\n",
    "    'logged_in', #sanity check, should be always 1 \n",
    "    'referer_class', # 'internal', 'external (search engine)', 'none', 'external'\n",
    "    'project_family', 'project', #sanity check, should always be Wikimedia Commons\n",
    "    'sp_createaccount', #sanity check, should always be 0\n",
    "    'user_wlm', #sanity check, should always be 1\n",
    "    'banners_seen', #count with cap at 10\n",
    "    'landing_seen', #sanity check, should be 1 mostly\n",
    "    'listpg_us_seen', 'listpg_de_seen'\n",
    "]\n",
    "def report_uniques (df):\n",
    "    print(datetime.datetime.now(), \"length of df:\", len(df))\n",
    "    for col in df.columns:\n",
    "        print(\"Values of\", col, \"are:\", df[col].unique())\n",
    "\n",
    "def get_cleaned_df(\n",
    "    df,\n",
    "    filter_col,\n",
    "    cols_out\n",
    "):\n",
    "    print(datetime.datetime.now(), 'starting export of csv for', filter_col)\n",
    "    df_out = df.loc[df[filter_col] > 0,cols_out]\n",
    "    df_out = df_out.sample(frac=1).reset_index(drop=True)\n",
    "    print(datetime.datetime.now(), 'done export of csv for', filter_col)\n",
    "    return(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_make_csv(\n",
    "    df_pd,\n",
    "    cols_out_bi = bi_cols_out,\n",
    "    cols_out_lp = lp_cols_out,\n",
    "    cols_out_ca = ca_cols_out,\n",
    "    cols_out_up = up_cols_out,\n",
    "    PATH_local = '/home/effeietsanders/shared_notebooks/', \n",
    "    FILE_out_base = 'output/csv/day_of_data_testing'\n",
    "):\n",
    "    print(\"--- converting bi data\", datetime.datetime.now())\n",
    "    df_bi = get_cleaned_df(df=df_pd,filter_col='bi_iswlm',cols_out=cols_out_bi)\n",
    "    report_uniques(df_bi)\n",
    "    df_bi.to_csv(FILE_out_base + \"_bi.csv\")\n",
    "    print(\"--- converting lp data\", datetime.datetime.now())\n",
    "    df_lp = get_cleaned_df(df=df_pd,filter_col='pg_landing',cols_out=cols_out_lp)\n",
    "    report_uniques(df_lp)\n",
    "    df_lp.to_csv(FILE_out_base + \"_lp.csv\")\n",
    "    print(\"--- converting ca data\", datetime.datetime.now())\n",
    "    df_ca = get_cleaned_df(df=df_pd,filter_col='sp_createaccount',cols_out=cols_out_ca)\n",
    "    report_uniques(df_ca)\n",
    "    df_ca.to_csv(FILE_out_base + \"_ca.csv\")\n",
    "    print(\"--- converting up data\", datetime.datetime.now())\n",
    "    df_up = get_cleaned_df(df=df_pd,filter_col='sp_upload',cols_out=cols_out_up)\n",
    "    report_uniques(df_up)\n",
    "    df_up.to_csv(FILE_out_base + \"_up.csv\")\n",
    "    print(\"success cleaning data and making csv's!\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_users(df):\n",
    "    print('preformatting grouping users', datetime.datetime.now())\n",
    "    df['bl_banner'] = df['bl_banner'].astype(\"category\")\n",
    "    print('grouping users', datetime.datetime.now())\n",
    "    df_grouped = df[df['user_wlm'] > 0][[\n",
    "        'user_id',\n",
    "        'user_wlm',\n",
    "        'logged_in', \n",
    "        'bl_banner',\n",
    "        'sp_createaccount',\n",
    "        'sp_upload',\n",
    "        'landing_seen',\n",
    "        'listpg_us_seen',\n",
    "        'listpg_de_seen'\n",
    "    ]].groupby('user_id')\n",
    "    print('aggregating', datetime.datetime.now())\n",
    "    df_out = pd.DataFrame()\n",
    "    print('aggregating logged_in', datetime.datetime.now())\n",
    "    df_out['logged_in'] = df_grouped['logged_in'].unique()\n",
    "#     print('aggregating bl_campaign', datetime.datetime.now())\n",
    "#     df_out['bl_campaign'] = df_grouped['bl_campaign'].unique()\n",
    "    print('aggregating bl_banner', datetime.datetime.now())\n",
    "    df_out['bl_banner'] = df_grouped['bl_banner'].unique()\n",
    "    print('aggregating sp_createaccount', datetime.datetime.now())\n",
    "    df_out['sp_createaccount'] = df_grouped['sp_createaccount'].max()\n",
    "    print('aggregating sp_upload', datetime.datetime.now())\n",
    "    df_out['sp_upload'] = df_grouped['sp_upload'].max()\n",
    "    print('aggregating landing_seen', datetime.datetime.now())\n",
    "    df_out['landing_seen'] = df_grouped['landing_seen'].max()\n",
    "    print('aggregating listpg_us_seen', datetime.datetime.now())\n",
    "    df_out['listpg_us_seen'] = df_grouped['listpg_us_seen'].max()\n",
    "    print('aggregating listpg_de_seen', datetime.datetime.now())\n",
    "    df_out['listpg_de_seen'] = df_grouped['listpg_de_seen'].max()\n",
    "    print('anonymizing', datetime.datetime.now())\n",
    "    df_out = df_out.sample(frac=1).reset_index(drop=True)\n",
    "    return(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_local_parquet(FILE_local):\n",
    "    localfile_exists = os.path.isdir(FILE_local)\n",
    "    if (localfile_exists):\n",
    "        print(datetime.datetime.now(), \"file exists locally\")\n",
    "        \n",
    "    else:\n",
    "        print(datetime.datetime.now(), \"file does not exist locally\")\n",
    "        return(\"file does not exist locally\")\n",
    "    os.system('rm -r ' + FILE_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXECUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_year = 2021\n",
    "exec_month = 10\n",
    "exec_day = 1\n",
    "\n",
    "print(datetime.datetime.now(), \"starting for \", exec_year, exec_month, exec_day)\n",
    "\n",
    "df_pd = get_day_of_data(\n",
    "    year=exec_year,\n",
    "    month=exec_month,\n",
    "    day=exec_day,\n",
    "    filename = \"day_of_data_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_make_csv(\n",
    "    df_pd = df_pd,\n",
    "    FILE_out_base = 'output/csv/day_of_data_' + str(exec_year) + str(exec_month).zfill(2) + str(exec_day).zfill(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_users = agg_users(df_pd)\n",
    "report_uniques(\n",
    "    df_agg_users[[\n",
    "        'sp_createaccount', \n",
    "        'sp_upload',\n",
    "        'landing_seen',    \n",
    "        'listpg_us_seen',\n",
    "        'listpg_de_seen'\n",
    "    ]]\n",
    ")\n",
    "\n",
    "df_agg_users.to_csv('output/csv/day_of_data_' + str(2021) + str(9).zfill(2) + str(22).zfill(2) + '_agg_users.csv')\n",
    "df_agg_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to manually check before deleting\n",
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_local_parquet(\n",
    "    FILE_local = 'output/csv/day_of_data_' + str(exec_year) + str(exec_month).zfill(2) + str(exec_day).zfill(2)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
